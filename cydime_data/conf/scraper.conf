[DatabaseOptions]
# database options - specify db_user, db_password, and db_name if using
# Postgresql or MySQL.  SQLite only needs the location of the db file
scraper_db_engine = sqlite
# ignore location if using Postgres/mysql
scraper_db_location = /path/to/scraper/db
# ignore the following options if using sqlite
scraper_db_user = scraperdbuser
scraper_db_passwd = password
scraper_db_name = auto_scraper_dev
scraper_db_host = localhost
# just leave at 0 if db is on localhost
scraper_db_port = 0
# this is the name used for the table holding daily scores
scraper_db_table = auto_scraper_db
scraper_table = scraper_db
expire_interval = 14
retire_interval = 60

scraper_tmp_engine = sqlite
# ignore location if using Postgres/mysql
scraper_tmp_location = /path/to/scraper/tmp
# ignore the following options if using sqlite
scraper_tmp_user = scrapertmpuser
scraper_tmp_passwd = password
scraper_tmp_name = auto_scraper_dev
scraper_tmp_host = localhost
# just leave at 0 if dwl_db is on localhost
scraper_tmp_port = 0
# this is the name used for the table holding daily scores
scraper_tmp_table = auto_scraper_tmp_db


[FileOptions]
data_dir = /path/to/cydime/data/dir
ip_list = preprocess/services.ext.features.Bytes

[ModelOptions]
label.ip_dom        = /data/dir/labels/IPDomsMap.csv  
label.ip_whois      = /data/dir/labels/IPWhoisMap.csv
label.dom_doc       = /data/dir/labels/DomDocsMap.csv
lexical.mission.sim = /data/dir/lexical/missionSims.csv

[ScraperOptions]
threshold = 200000
dst_dir = /path/to/scraper/data/dir
java_path = /usr/bin/java
mallet_path = /path/to/mallet/mallet-2.0.7


[CrawlerOptions]
# crawler paths
crawler.path		= /path/to/scraper/data/
crawler.web.path	= /path/to/scraper/data/web
crawler.web.raw		= /path/to/scraper/data/web/crawl_raw
crawler.web.output	= /path/to/scraper/data/web/crawl_text
crawler.web.merged	= /path/to/scraper/data/web/crawl_merged
crawler.web.mallet	= /path/to/scraper/data/web.mallet

crawler.mission.path	= /path/to/scraper/data/mission
crawler.mission.domains = /path/to/scraper/data/missionDoms.csv
crawler.mission.raw	    = /path/to/scraper/data/mission/crawl_raw
crawler.mission.output	= /path/to/scraper/data/mission/crawl_text
crawler.mission.merged	= /path/to/scraper/data/mission/crawl_merged
crawler.mission.mallet	= /path/to/scraper/data/mission.mallet
# crawler options
crawler.max_crawlers		= 200
crawler.threads_per_crawler	= 1
crawler.max_pages_per_host	= 20
crawler.politeness_delay	= 2000
# incremental output paths
crawler.ip_dom		= /path/to/cydime/data/labels/IPDomsMap.csv
crawler.ip_whois	= /path/to/cydime/data/labels/IPWhoisMap.csv
crawler.dom_doc		= /path/to/cydime/data/labels/DomDocsMap.csv
crawler.mission.sim	= /path/to/cydime/data/lexical/missionSims.csv

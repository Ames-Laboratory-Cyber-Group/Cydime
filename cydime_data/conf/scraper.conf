[DatabaseOptions]
# database options - specify db_user, db_password, and db_name if using
# Postgresql or MySQL.  SQLite only needs the location of the db file
scraper_db_engine = sqlite
# ignore location if using Postgres/mysql
scraper_db_location = /cydime_data/scrape/scraper.db
# ignore the following options if using sqlite
scraper_db_user = scraperdbuser
scraper_db_passwd = password
scraper_db_name = auto_scraper_dev
scraper_db_host = localhost
# just leave at 0 if db is on localhost
scraper_db_port = 0
# this is the name used for the table holding daily scores
scraper_db_table = auto_scraper_db
scraper_table = scraper_db
expire_interval = 14
retire_interval = 60

scraper_tmp_engine = sqlite
# ignore location if using Postgres/mysql
scraper_tmp_location = /cydime_data/scrape/scraper_tmp.db
# ignore the following options if using sqlite
scraper_tmp_user = scrapertmpuser
scraper_tmp_passwd = password
scraper_tmp_name = auto_scraper_dev
scraper_tmp_host = localhost
# just leave at 0 if dwl_db is on localhost
scraper_tmp_port = 0
# this is the name used for the table holding daily scores
scraper_tmp_table = auto_scraper_tmp_db

[FileOptions]
data_dir = /cydime_data/data
ip_list = preprocess/services.ext.features.Bytes

[ModelOptions]
label.ip_dom        = /cydime_data/data/labels/IPDomsMap.csv  
label.ip_whois      = /cydime_data/data/labels/IPWhoisMap.csv
label.dom_doc       = /cydime_data/data/labels/DomDocsMap.csv
lexical.mission.sim = /cydime_data/data/lexical/missionSims.csv

[ScraperOptions]
threshold = 200000
dst_dir = /cydime_data/scrape
java_path = /usr/bin/java
mallet_path = /cydime_data/bin/mallet

[CrawlerOptions]
# crawler paths
crawler.path		= /cydime_data/scrape
crawler.web.path	= /cydime_data/scrape/web
crawler.web.raw		= /cydime_data/scrape/web/crawl_raw
crawler.web.output	= /cydime_data/scrape/web/crawl_text
crawler.web.merged	= /cydime_data/scrape/web/crawl_merged
crawler.web.mallet	= /cydime_data/scrape/web.mallet

crawler.mission.path	= /cydime_data/scrape/mission
crawler.mission.domains = /cydime_data/data/lexical/missionDoms.csv
crawler.mission.raw	    = /cydime_data/scrape/mission/crawl_raw
crawler.mission.output	= /cydime_data/scrape/mission/crawl_text
crawler.mission.merged	= /cydime_data/scrape/mission/crawl_merged
# crawler options
crawler.max_crawlers		= 200
crawler.threads_per_crawler	= 1
crawler.max_pages_per_host	= 20
crawler.politeness_delay	= 2000
# incremental output paths
crawler.ip_dom		= /cydime_data/scrape/IPDomsMap.csv
crawler.ip_whois	= /cydime_data/scrape/IPWhoisMap.csv
crawler.dom_doc		= /cydime_data/scrape/DomDocsMap.csv
crawler.mission.sim	= /cydime_data/scrape/missionSims.csv
